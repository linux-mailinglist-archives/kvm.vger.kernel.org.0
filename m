Return-Path: <kvm-owner@vger.kernel.org>
X-Original-To: lists+kvm@lfdr.de
Delivered-To: lists+kvm@lfdr.de
Received: from out1.vger.email (out1.vger.email [IPv6:2620:137:e000::1:20])
	by mail.lfdr.de (Postfix) with ESMTP id 9FEB4609F0A
	for <lists+kvm@lfdr.de>; Mon, 24 Oct 2022 12:29:36 +0200 (CEST)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S229674AbiJXK3e (ORCPT <rfc822;lists+kvm@lfdr.de>);
        Mon, 24 Oct 2022 06:29:34 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:32854 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S230141AbiJXK3c (ORCPT <rfc822;kvm@vger.kernel.org>);
        Mon, 24 Oct 2022 06:29:32 -0400
Received: from ams.source.kernel.org (ams.source.kernel.org [IPv6:2604:1380:4601:e00::1])
        by lindbergh.monkeyblade.net (Postfix) with ESMTPS id 5540426E8
        for <kvm@vger.kernel.org>; Mon, 24 Oct 2022 03:29:28 -0700 (PDT)
Received: from smtp.kernel.org (relay.kernel.org [52.25.139.140])
        (using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256 bits))
        (No client certificate requested)
        by ams.source.kernel.org (Postfix) with ESMTPS id 57BB2B810D9
        for <kvm@vger.kernel.org>; Mon, 24 Oct 2022 10:29:27 +0000 (UTC)
Received: by smtp.kernel.org (Postfix) with ESMTPSA id EE13AC433C1;
        Mon, 24 Oct 2022 10:29:25 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=kernel.org;
        s=k20201202; t=1666607366;
        bh=e9EJjLUy6t6gwdKoVDL3bho6pQx55paAnahTf23c/+I=;
        h=Date:From:To:Cc:Subject:In-Reply-To:References:From;
        b=V5KSuSnkGIMXtpKaGL98PrEKJ9aQkHuz0r24lmwoCxFg6Ps/6RuaIyw8MPVoEEnJV
         WuQZE+qJyjyIoT8R000LLhkrk93xsxyMcLae7lKIcPUv6LS0WImMy3+FhzDHcLonpT
         WjSzZLadY4L+snHKyFblPA1QQ7uWkcpZqMA+P1vVxLfH7LhgD7cXvj9QuaGjycCjfX
         UTaJBGcG+0FHhgTMbdQ5PVINv5Y0ikRXakPxKYDFIzArgxjgiTokH9yQ9VT753eiwu
         DsVtouLX70chZw1YZ9h39HGP0i/5u/YEqsfbIPdx5/eCJb0kcgpo6kirMTosftJtTT
         NCBJBTiM3NyUw==
Received: from sofa.misterjones.org ([185.219.108.64] helo=goblin-girl.misterjones.org)
        by disco-boy.misterjones.org with esmtpsa  (TLS1.3) tls TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
        (Exim 4.95)
        (envelope-from <maz@kernel.org>)
        id 1omuhv-0017qA-Kt;
        Mon, 24 Oct 2022 11:29:23 +0100
Date:   Mon, 24 Oct 2022 11:29:23 +0100
Message-ID: <86zgdlms58.wl-maz@kernel.org>
From:   Marc Zyngier <maz@kernel.org>
To:     Reiji Watanabe <reijiw@google.com>
Cc:     Linux ARM <linux-arm-kernel@lists.infradead.org>,
        kvmarm@lists.cs.columbia.edu, kvm@vger.kernel.org,
        kernel-team@android.com
Subject: Re: [PATCH 1/9] KVM: arm64: PMU: Align chained counter implementation with architecture pseudocode
In-Reply-To: <CAAeT=Fz55H09PWpmMu1sBkV=iUEHWezwhghJskaWAoqQsi2N0A@mail.gmail.com>
References: <20220805135813.2102034-1-maz@kernel.org>
        <20220805135813.2102034-2-maz@kernel.org>
        <CAAeT=Fz55H09PWpmMu1sBkV=iUEHWezwhghJskaWAoqQsi2N0A@mail.gmail.com>
User-Agent: Wanderlust/2.15.9 (Almost Unreal) SEMI-EPG/1.14.7 (Harue)
 FLIM-LB/1.14.9 (=?UTF-8?B?R29qxY0=?=) APEL-LB/10.8 EasyPG/1.0.0 Emacs/27.1
 (aarch64-unknown-linux-gnu) MULE/6.0 (HANACHIRUSATO)
MIME-Version: 1.0 (generated by SEMI-EPG 1.14.7 - "Harue")
Content-Type: text/plain; charset=US-ASCII
X-SA-Exim-Connect-IP: 185.219.108.64
X-SA-Exim-Rcpt-To: reijiw@google.com, linux-arm-kernel@lists.infradead.org, kvmarm@lists.cs.columbia.edu, kvm@vger.kernel.org, kernel-team@android.com
X-SA-Exim-Mail-From: maz@kernel.org
X-SA-Exim-Scanned: No (on disco-boy.misterjones.org); SAEximRunCond expanded to false
X-Spam-Status: No, score=-7.6 required=5.0 tests=BAYES_00,DKIMWL_WL_HIGH,
        DKIM_SIGNED,DKIM_VALID,DKIM_VALID_AU,DKIM_VALID_EF,RCVD_IN_DNSWL_HI,
        SPF_HELO_NONE,SPF_PASS autolearn=ham autolearn_force=no version=3.4.6
X-Spam-Checker-Version: SpamAssassin 3.4.6 (2021-04-09) on
        lindbergh.monkeyblade.net
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org

Hi Reiji,

Catching up on this.

On Tue, 23 Aug 2022 05:30:21 +0100,
Reiji Watanabe <reijiw@google.com> wrote:
> 
> Hi Marc,
> 
> On Fri, Aug 5, 2022 at 6:58 AM Marc Zyngier <maz@kernel.org> wrote:
> >
> > Ricardo recently pointed out that the PMU chained counter emulation
> > in KVM wasn't quite behaving like the one on actual hardware, in
> > the sense that a chained counter would expose an overflow on
> > both halves of a chained counter, while KVM would only expose the
> > overflow on the top half.
> >
> > The difference is subtle, but significant. What does the architecture
> > say (DDI0087 H.a):
> >
> > - Before PMUv3p4, all counters but the cycle counter are 32bit
> > - A 32bit counter that overflows generates a CHAIN event on the
> >   adjacent counter after exposing its own overflow status
> > - The CHAIN event is accounted if the counter is correctly
> >   configured (CHAIN event selected and counter enabled)
> >
> > This all means that our current implementation (which uses 64bit
> > perf events) prevents us from emulating this overflow on the lower half.
> >
> > How to fix this? By implementing the above, to the letter.
> >
> > This largly results in code deletion, removing the notions of
> > "counter pair", "chained counters", and "canonical counter".
> > The code is further restructured to make the CHAIN handling similar
> > to SWINC, as the two are now extremely similar in behaviour.
> >
> > Reported-by: Ricardo Koller <ricarkol@google.com>
> > Signed-off-by: Marc Zyngier <maz@kernel.org>
> > ---
> >  arch/arm64/kvm/pmu-emul.c | 324 +++++++++++---------------------------
> >  include/kvm/arm_pmu.h     |   2 -
> >  2 files changed, 91 insertions(+), 235 deletions(-)
> >
> > diff --git a/arch/arm64/kvm/pmu-emul.c b/arch/arm64/kvm/pmu-emul.c
> > index 11c43bed5f97..4986e8b3ea6c 100644
> > --- a/arch/arm64/kvm/pmu-emul.c
> > +++ b/arch/arm64/kvm/pmu-emul.c

[...]

> > +/*
> > + * Perform an increment on any of the counters described in @mask,
> > + * generating the overflow if required, and propagate it as a chained
> > + * event if possible.
> > + */
> > +static void kvm_pmu_counter_increment(struct kvm_vcpu *vcpu,
> > +                                     unsigned long mask, u32 event)
> > +{
> > +       int i;
> > +
> > +       if (!kvm_vcpu_has_pmu(vcpu))
> > +               return;
> > +
> > +       if (!(__vcpu_sys_reg(vcpu, PMCR_EL0) & ARMV8_PMU_PMCR_E))
> > +               return;
> > +
> > +       /* Weed out disabled counters */
> > +       mask &= __vcpu_sys_reg(vcpu, PMCNTENSET_EL0);
> > +
> > +       for_each_set_bit(i, &mask, ARMV8_PMU_CYCLE_IDX) {
> > +               u64 type, reg;
> > +
> > +               /* Filter on event type */
> > +               type = __vcpu_sys_reg(vcpu, PMEVTYPER0_EL0 + i);
> > +               type &= kvm_pmu_event_mask(vcpu->kvm);
> > +               if (type != event)
> > +                       continue;
> > +
> > +               /* Increment this counter */
> > +               reg = __vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i) + 1;
> > +               reg = lower_32_bits(reg);
> > +               __vcpu_sys_reg(vcpu, PMEVCNTR0_EL0 + i) = reg;
> > +
> > +               if (reg) /* No overflow? move on */
> > +                       continue;
> > +
> > +               /* Mark overflow */
> > +               __vcpu_sys_reg(vcpu, PMOVSSET_EL0) |= BIT(i);
> 
> Perhaps it might be useful to create another helper that takes
> care of just one counter (it would essentially do the code above
> in the loop). The helper could be used (in addition to the above
> loop) from the code below for the CHAIN event case and from
> kvm_pmu_perf_overflow(). Then unnecessary execution of
> for_each_set_bit() could be avoided for these two cases.

I'm not sure it really helps. We would still need to check whether the
counter is enabled, and we'd need to bring that into the helper
instead of keeping it outside of the loop.

[...]

> > @@ -625,30 +528,27 @@ static void kvm_pmu_create_perf_event(struct kvm_vcpu *vcpu, u64 select_idx)
> >  {
> >         struct arm_pmu *arm_pmu = vcpu->kvm->arch.arm_pmu;
> >         struct kvm_pmu *pmu = &vcpu->arch.pmu;
> > -       struct kvm_pmc *pmc;
> > +       struct kvm_pmc *pmc = &pmu->pmc[select_idx];
> >         struct perf_event *event;
> >         struct perf_event_attr attr;
> >         u64 eventsel, counter, reg, data;
> >
> > -       /*
> > -        * For chained counters the event type and filtering attributes are
> > -        * obtained from the low/even counter. We also use this counter to
> > -        * determine if the event is enabled/disabled.
> > -        */
> > -       pmc = kvm_pmu_get_canonical_pmc(&pmu->pmc[select_idx]);
> > -
> > -       reg = (pmc->idx == ARMV8_PMU_CYCLE_IDX)
> > +       reg = (select_idx == ARMV8_PMU_CYCLE_IDX)
> >               ? PMCCFILTR_EL0 : PMEVTYPER0_EL0 + pmc->idx;
> 
> You may want to use select_idx instead of pmc->id for consistency ?

Yes. Although Oliver had a point in saying that these pmc->idx vs
select_idx conversions were not strictly necessary and cluttered the
patch.

[...]

> > @@ -752,11 +607,15 @@ static void kvm_pmu_update_pmc_chained(struct kvm_vcpu *vcpu, u64 select_idx)
> >  void kvm_pmu_set_counter_event_type(struct kvm_vcpu *vcpu, u64 data,
> >                                     u64 select_idx)
> >  {
> > +       struct kvm_pmu *pmu = &vcpu->arch.pmu;
> > +       struct kvm_pmc *pmc = &pmu->pmc[select_idx];
> >         u64 reg, mask;
> >
> >         if (!kvm_vcpu_has_pmu(vcpu))
> >                 return;
> >
> > +       kvm_pmu_stop_counter(vcpu, pmc);
> 
> It appears that kvm_pmu_stop_counter() doesn't have to be called here
> because it is called in the beginning of kvm_pmu_create_perf_event().

It feels a bit odd to change the event type without stopping the
counter first, but I can't see anything going wrong if we omit it.

I'll drop it.

Thanks,

	M.

-- 
Without deviation from the norm, progress is not possible.
