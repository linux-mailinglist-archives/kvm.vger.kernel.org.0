Return-Path: <kvm-owner@vger.kernel.org>
X-Original-To: lists+kvm@lfdr.de
Delivered-To: lists+kvm@lfdr.de
Received: from out1.vger.email (out1.vger.email [IPv6:2620:137:e000::1:20])
	by mail.lfdr.de (Postfix) with ESMTP id 8852E4E1F3F
	for <lists+kvm@lfdr.de>; Mon, 21 Mar 2022 04:13:41 +0100 (CET)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
        id S238723AbiCUDPB (ORCPT <rfc822;lists+kvm@lfdr.de>);
        Sun, 20 Mar 2022 23:15:01 -0400
Received: from lindbergh.monkeyblade.net ([23.128.96.19]:42444 "EHLO
        lindbergh.monkeyblade.net" rhost-flags-OK-OK-OK-OK) by vger.kernel.org
        with ESMTP id S234851AbiCUDO7 (ORCPT <rfc822;kvm@vger.kernel.org>);
        Sun, 20 Mar 2022 23:14:59 -0400
X-Greylist: delayed 71 seconds by postgrey-1.37 at lindbergh.monkeyblade.net; Sun, 20 Mar 2022 20:13:34 PDT
Received: from chinatelecom.cn (prt-mail.chinatelecom.cn [42.123.76.226])
        by lindbergh.monkeyblade.net (Postfix) with ESMTP id 0C202140E0
        for <kvm@vger.kernel.org>; Sun, 20 Mar 2022 20:13:32 -0700 (PDT)
HMM_SOURCE_IP: 172.18.0.218:47140.1415164590
HMM_ATTACHE_NUM: 0000
HMM_SOURCE_TYPE: SMTP
Received: from clientip-36.111.64.85 (unknown [172.18.0.218])
        by chinatelecom.cn (HERMES) with SMTP id 3A2E42800FB;
        Mon, 21 Mar 2022 11:13:23 +0800 (CST)
X-189-SAVE-TO-SEND: wucy11@chinatelecom.cn
Received: from  ([172.18.0.218])
        by app0025 with ESMTP id 720c90debc374da4ab78b2700e9cc5cc for kvm@vger.kernel.org;
        Mon, 21 Mar 2022 11:13:31 CST
X-Transaction-ID: 720c90debc374da4ab78b2700e9cc5cc
X-Real-From: wucy11@chinatelecom.cn
X-Receive-IP: 172.18.0.218
X-MEDUSA-Status: 0
Sender: wucy11@chinatelecom.cn
Message-ID: <3455c9de-3d2f-08e0-5194-d741034a3e44@chinatelecom.cn>
Date:   Mon, 21 Mar 2022 11:13:21 +0800
MIME-Version: 1.0
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101
 Thunderbird/91.7.0
From:   Chongyun Wu <wucy11@chinatelecom.cn>
Subject: [PATCH 3/5] kvm: Dirty ring autoconverge optmization for
 kvm_cpu_synchronize_kick_all
To:     kvm@vger.kernel.org, qemu-devel@nongnu.org
Cc:     Paolo Bonzini <pbonzini@redhat.com>, Peter Xu <peterx@redhat.com>,
        Richard Henderson <richard.henderson@linaro.org>,
        Juan Quintela <quintela@redhat.com>,
        "Dr. David Alan Gilbert" <dgilbert@redhat.com>,
        David Hildenbrand <david@redhat.com>,
        =?UTF-8?Q?Philippe_Mathieu-Daud=c3=a9?= <f4bug@amsat.org>,
        yubin1@chinatelecom.cn,
        "ligh10@chinatelecom.cn" <ligh10@chinatelecom.cn>,
        zhengwenm@chinatelecom.cn
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Spam-Status: No, score=-1.9 required=5.0 tests=BAYES_00,SPF_HELO_PASS,
        SPF_PASS,T_SCC_BODY_TEXT_LINE autolearn=ham autolearn_force=no
        version=3.4.6
X-Spam-Checker-Version: SpamAssassin 3.4.6 (2021-04-09) on
        lindbergh.monkeyblade.net
Precedence: bulk
List-ID: <kvm.vger.kernel.org>
X-Mailing-List: kvm@vger.kernel.org

Dirty ring feature need call kvm_cpu_synchronize_kick_all
to flush hardware buffers into KVMslots, but when aucoverge
run kvm_cpu_synchronize_kick_all calling will become more
and more time consuming. This will significantly reduce the
efficiency of dirty page queries, especially when memory
pressure is high and the speed limit is high.

When the CPU speed limit is high and kvm_cpu_synchronize_kick_all
is time-consuming, the rate of dirty pages generated by the VM
will also be significantly reduced, so it is not necessary to
call kvm_cpu_synchronize_kick_all at this time, just call it once
before stopping the VM. This will significantly improve the
efficiency of dirty page queries under high pressure.

Signed-off-by: Chongyun Wu <wucy11@chinatelecom.cn>
---
  accel/kvm/kvm-all.c   | 23 +++--------------------
  include/sysemu/cpus.h |  1 +
  migration/migration.c | 12 ++++++++++++
  softmmu/cpus.c        | 18 ++++++++++++++++++
  4 files changed, 34 insertions(+), 20 deletions(-)

diff --git a/accel/kvm/kvm-all.c b/accel/kvm/kvm-all.c
index 51012f4..64a211b 100644
--- a/accel/kvm/kvm-all.c
+++ b/accel/kvm/kvm-all.c
@@ -839,25 +839,6 @@ static uint64_t kvm_dirty_ring_reap(KVMState *s)
      return total;
  }

-static void do_kvm_cpu_synchronize_kick(CPUState *cpu, run_on_cpu_data arg)
-{
-    /* No need to do anything */
-}
-
-/*
- * Kick all vcpus out in a synchronized way.  When returned, we
- * guarantee that every vcpu has been kicked and at least returned to
- * userspace once.
- */
-static void kvm_cpu_synchronize_kick_all(void)
-{
-    CPUState *cpu;
-
-    CPU_FOREACH(cpu) {
-        run_on_cpu(cpu, do_kvm_cpu_synchronize_kick, RUN_ON_CPU_NULL);
-    }
-}
-
  /*
   * Flush all the existing dirty pages to the KVM slot buffers.  When
   * this call returns, we guarantee that all the touched dirty pages
@@ -879,7 +860,9 @@ static void kvm_dirty_ring_flush(void)
       * First make sure to flush the hardware buffers by kicking all
       * vcpus out in a synchronous way.
       */
-    kvm_cpu_synchronize_kick_all();
+    if (!cpu_throttle_get_percentage()) {
+        qemu_kvm_cpu_synchronize_kick_all();
+    }
      kvm_dirty_ring_reap(kvm_state);
      trace_kvm_dirty_ring_flush(1);
  }
diff --git a/include/sysemu/cpus.h b/include/sysemu/cpus.h
index 868f119..3225b27 100644
--- a/include/sysemu/cpus.h
+++ b/include/sysemu/cpus.h
@@ -56,5 +56,6 @@ extern int smp_threads;
  #endif

  void list_cpus(const char *optarg);
+void qemu_kvm_cpu_synchronize_kick_all(void);

  #endif
diff --git a/migration/migration.c b/migration/migration.c
index bcc385b..1114b2f 100644
--- a/migration/migration.c
+++ b/migration/migration.c
@@ -61,6 +61,8 @@
  #include "sysemu/cpus.h"
  #include "yank_functions.h"
  #include "sysemu/qtest.h"
+#include "sysemu/kvm.h"
+#include "sysemu/cpus.h"

  #define MAX_THROTTLE  (128 << 20)      /* Migration transfer speed throttling */

@@ -3177,6 +3179,16 @@ static void migration_completion(MigrationState *s)

          if (!ret) {
              bool inactivate = !migrate_colo_enabled();
+            /*
+             * Before stop vm do qemu_kvm_cpu_synchronize_kick_all to
+             * fulsh hardware buffer into KVMslots for dirty ring
+             * optmiaztion, If qemu_kvm_cpu_synchronize_kick_all is not
+             * called when the CPU speed is limited to improve efficiency
+             */
+            if (kvm_dirty_ring_enabled()
+                && cpu_throttle_get_percentage()) {
+                qemu_kvm_cpu_synchronize_kick_all();
+            }
              ret = vm_stop_force_state(RUN_STATE_FINISH_MIGRATE);
              trace_migration_completion_vm_stop(ret);
              if (ret >= 0) {
diff --git a/softmmu/cpus.c b/softmmu/cpus.c
index 035395a..505ed3e 100644
--- a/softmmu/cpus.c
+++ b/softmmu/cpus.c
@@ -807,3 +807,21 @@ void qmp_inject_nmi(Error **errp)
      nmi_monitor_handle(monitor_get_cpu_index(monitor_cur()), errp);
  }

+static void do_kvm_cpu_synchronize_kick(CPUState *cpu, run_on_cpu_data arg)
+{
+    /* No need to do anything */
+}
+
+/*
+ * Kick all vcpus out in a synchronized way.  When returned, we
+ * guarantee that every vcpu has been kicked and at least returned to
+ * userspace once.
+ */
+void qemu_kvm_cpu_synchronize_kick_all(void)
+{
+    CPUState *cpu;
+
+    CPU_FOREACH(cpu) {
+        run_on_cpu(cpu, do_kvm_cpu_synchronize_kick, RUN_ON_CPU_NULL);
+    }
+}
--
1.8.3.1

-- 
Best Regard,
Chongyun Wu
